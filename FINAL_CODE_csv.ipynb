{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "f = open('Example.c', 'r')\n",
    "program = f.read()\n",
    "count = 0\n",
    "\n",
    "#with open('Final_Output.txt','w') as f:\n",
    " #   f.write(\"-------------------------------------------------------CODE--------------------------------------------------------------------------\\n\\n\")\n",
    "  #  f.write(program)\n",
    "\n",
    "Identifiers_Output = []\n",
    "Keywords_Output = []\n",
    "Symbols_Output = []\n",
    "Operators_Output = []\n",
    "Numerals_Output = []\n",
    "Headers_Output = []\n",
    "\n",
    "def remove_Spaces(program):\n",
    "    scanned_Program = []\n",
    "    for line in prog:\n",
    "        if (line.strip() != ''):\n",
    "            scanned_Program.append(line.strip())\n",
    "    return scanned_Program\n",
    "\n",
    "\n",
    "def remove_Comments(program):\n",
    "    program_Multi_Comments_Removed = re.sub(\"/\\*[^*]*\\*+(?:[^/*][^*]*\\*+)*/\", \"\", program)\n",
    "    program_Single_Comments_Removed = re.sub(\"//.*\", \"\", program_Multi_Comments_Removed)\n",
    "    program_Comments_removed = program_Single_Comments_Removed\n",
    "    return program_Comments_removed\n",
    "\n",
    "\n",
    "\n",
    "RE_Keywords = \"auto|break|case|char|const|continue|default|do|double|else|extern|float|for|enum|goto|if|int|long|register|return|short|signed|sizeof|static|struct|switch|typedef|union|unsigned|void|volatile|while|string|class|struc|include\"\n",
    "RE_Operators = \"(\\++)|(-)|(=)|(\\*)|(/)|(%)|(--)|(<=)|(>=)\"\n",
    "RE_Numerals = \"^(\\d+)$\"\n",
    "RE_Special_Characters = \"[\\[@&~!#$\\^\\|{}\\]:;<>?,\\.']|\\(\\)|\\(|\\)|{}|\\[\\]|\\\"\"\n",
    "RE_Identifiers = \"^[a-zA-Z_]+[a-zA-Z0-9_]*\"\n",
    "RE_Headers = \"([a-zA-Z]+\\.[h])\"\n",
    "\n",
    "\n",
    "program_Comments_removed = remove_Comments(program)\n",
    "prog = program_Comments_removed.split('\\n')\n",
    "\n",
    "\n",
    "scanned_Prog = remove_Spaces(program_Comments_removed)\n",
    "#scanned_Program = '\\n'.join([str(elem) for elem in scanned_Prog])\n",
    "\n",
    "\n",
    "\n",
    "#match_counter = 0\n",
    "\n",
    "\n",
    "Source_Code=[]\n",
    "for line in scanned_Prog:\n",
    "        Source_Code.append(line)\n",
    "       \n",
    "        \n",
    "#display_counter = 0\n",
    "for line in Source_Code:\n",
    "    count = count + 1\n",
    "    if(line.startswith(\"#include\")):\n",
    "        tokens = nltk.word_tokenize(line)\n",
    "    else:\n",
    "        tokens = nltk.wordpunct_tokenize(line)\n",
    "    for token in tokens:\n",
    "        if(re.findall(RE_Keywords, token)):\n",
    "            Keywords_Output.append(token)\n",
    "        elif(re.findall(RE_Headers,token)):\n",
    "            Headers_Output.append(token)\n",
    "        elif(re.findall(RE_Operators, token)):\n",
    "            Operators_Output.append(token)\n",
    "        elif(re.findall(RE_Numerals,token)):\n",
    "            Numerals_Output.append(token)\n",
    "        elif (re.findall(RE_Special_Characters, token)):\n",
    "            Symbols_Output.append(token)\n",
    "        elif (re.findall(RE_Identifiers, token)):\n",
    "            Identifiers_Output.append(token)\n",
    "\n",
    "import csv\n",
    "\n",
    "csv.reader?\n",
    "            \n",
    "\n",
    "#with open('Final_Output.txt','a') as f:\n",
    " #   f.write('\\n\\n----------------------------------------------------------TOKENS----------------------------------------------------------------\\n\\n')\n",
    "  #  f.write(\"There Are \"+ str(len(Keywords_Output)) +\" Keywords: \")\n",
    "   # f.writelines(str(Keywords_Output)+'\\n\\n')\n",
    "    \n",
    "   # f.write(\"There Are \"+ str(len(Identifiers_Output)) +\" Identifiers: \")\n",
    "    #f.write(str(Identifiers_Output)+'\\n\\n')\n",
    "    \n",
    "   # f.write(\"There Are \"+ str(len(Headers_Output)) +\" Header Files: \")\n",
    "    #f.write(str(Headers_Output)+'\\n\\n')\n",
    "    \n",
    "    #f.write(\"There Are \"+ str(len(Symbols_Output)) +\" Symbols: \")\n",
    "    #f.write(str(Symbols_Output)+'\\n\\n')\n",
    "    \n",
    "    #f.write(\"There Are \"+ str(len(Numerals_Output)) +\" Numerals: \")\n",
    "    #f.write(str(Numerals_Output)+'\\n\\n')\n",
    "    \n",
    "\n",
    "#print(\"There Are \",len(Keywords_Output),\"Keywords: \",Keywords_Output)\n",
    "#print(\"\\n\")\n",
    "#print(\"There Are \",len(Identifiers_Output),\"Identifiers: \",Identifiers_Output)\n",
    "#print(\"\\n\")\n",
    "#print(\"There Are \",len(Headers_Output),\"Header Files: \",Headers_Output)\n",
    "#print(\"\\n\")\n",
    "#print(\"There Are\",len(Symbols_Output),\"Symbols:\",Symbols_Output)\n",
    "#print(\"\\n\")\n",
    "#print(\"There Are \",len(Numerals_Output),\"Numerals:\",Numerals_Output)\n",
    "#print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "str.join?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "str.split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "['']\n",
      "['include']\n",
      "['int']\n",
      "['int']\n",
      "['printf']\n",
      "['if']\n",
      "['printf']\n",
      "['else']\n",
      "['if']\n",
      "['printf']\n",
      "['else']\n",
      "['if']\n",
      "['printf']\n",
      "['else']\n",
      "['printf']\n",
      "['return']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['400']\n",
      "['0']\n",
      "['100']\n",
      "['0']\n",
      "['4']\n",
      "['0']\n",
      "['0']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('CSV.csv') as f:\n",
    "    a=csv.reader(f)\n",
    "    for b in a:\n",
    "        print(b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('CSV.csv','w',newline='') as f:\n",
    "    c=csv.writer(f,delimiter=',')\n",
    "    c.writerow([''])\n",
    "    c.writerow([''])\n",
    "    for output in Keywords_Output:\n",
    "        c.writerow([output])\n",
    "    c.writerow([''])\n",
    "    c.writerow([''])\n",
    "    c.writerow([''])\n",
    "    c.writerow([''])\n",
    "    \n",
    "    for output in Numerals_Output:\n",
    "        c.writerow([output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^[a-zA-Z_]+[a-zA-Z0-9_]*\n"
     ]
    }
   ],
   "source": [
    "print(RE_Identifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
